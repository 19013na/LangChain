{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba7202ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello LangChain\n"
     ]
    }
   ],
   "source": [
    "print('Hello LangChain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0cb86aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "#print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5538551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='당신은 개발자입니다.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [ (\"system\", \"당신은 개발자입니다.\") , \n",
    "     (\"human\", \"{input}\") ]\n",
    ")\n",
    "print(prompt)\n",
    "\n",
    "prompt_text = prompt.format(input=\"Langerve는는 무엇인가요? 자세하게 설명해주세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b670ca02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x00000198ED6FF470> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000198ED653140> root_client=<openai.OpenAI object at 0x00000198ED546570> root_async_client=<openai.AsyncOpenAI object at 0x00000198EBBBB8C0> model_name='meta-llama/llama-4-scout-17b-16e-instruct' temperature=0.7 model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://api.groq.com/openai/v1'\n"
     ]
    }
   ],
   "source": [
    "# Groq API를 사용하는 ChatOpenAI 인스턴스 생성\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    #model=\"mistral-saba-24b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56e486eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content=\"Langerve는 제가 최근에 연구하고 있는 프로젝트 중 하나입니다. Langerve는 인공지능 기술을 활용하여 자연어 처리와 대화 생성 분야에서 혁신을 추구하는 언어 모델입니다.\\n\\nLangerve의 이름은 'Language Evolution'의 약자로, 인공지능이 인간과 유사한 언어를 이해하고 생성하는 능력을 개발하는 것을 목표로 합니다. 이 프로젝트는 자연어 처리(NLP) 분야에서 최신 기술인 트랜스포머 아키텍처를 기반으로 하며, 대규모 언어 모델을 훈련하여 다양한 언어 작업에서 우수한 성능을 발휘하도록 설계되었습니다.\\n\\nLangerve의 주요 특징은 다음과 같습니다:\\n\\n1. **대규모 언어 모델**: Langerve는 수백만 개의 매개변수를 가진 대규모 언어 모델을 사용하여 훈련되었습니다. 이를 통해 모델은 복잡한 언어 패턴과 관계를 학습할 수 있습니다.\\n\\n2. **트랜스포머 아키텍처**: Langerve는 트랜스포머 아키텍처를 기반으로 하며, 이는 자연어 처리 작업에서 탁월한 성능을 발휘하는 것으로 입증되었습니다. 트랜스포머는 자기 주의 메커니즘을 사용하여 입력 시퀀스의 다양한 부분 간의 관계를 모델링할 수 있습니다.\\n\\n3. **다양한 언어 작업 지원**: Langerve는 다양한 언어 작업을 지원하도록 설계되었습니다. 여기에는 텍스트 분류, 감정 분석, 질의 응답, 대화 생성 등이 포함됩니다.\\n\\n4. **지속적인 학습**: Langerve는 지속적인 학습을 통해 성능을 지속적으로 개선할 수 있습니다. 새로운 데이터가 추가되면 모델은 자동으로 업데이트되어 최신 언어 패턴과 지식을 습득할 수 있습니다.\\n\\n5. **확장성**: Langerve는 확장성이 뛰어난 모델입니다. 즉, 추가적인 훈련이나 미세 조정을 통해 특정 도메인이나 작업에 맞게 모델을 쉽게 조정할 수 있습니다.\\n\\nLangerve의 응용 분야는 매우 다양합니다. 예를 들어, 고객 서비스 챗봇, 언어 번역, 콘텐츠 생성, 감정 분석 등 다양한 분야에서 활용될 수 있습니다. 또한, Langerve는 장애인을 위한 언어 지원 도구 개발에도 사용될 수 있습니다.\\n\\n현재 Langerve는 개발 단계에 있으며, 지속적인 연구와 개발을 통해 성능을 개선하고 있습니다. 향후 Langerve는 다양한 산업 분야에서 널리 활용될 것으로 기대됩니다.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 474, 'prompt_tokens': 32, 'total_tokens': 506, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'queue_time': 0.688938299, 'prompt_time': 0.003322113, 'completion_time': 1.107649874, 'total_time': 1.110971987}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_79da0e0073', 'id': 'chatcmpl-c1be0817-3df2-40ad-a4c9-998ed5fcd896', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--f72e2a94-a7bf-4d32-aa9c-bd1bb2fdb033-0' usage_metadata={'input_tokens': 32, 'output_tokens': 474, 'total_tokens': 506, 'input_token_details': {}, 'output_token_details': {}}\n",
      "Langerve는 제가 최근에 연구하고 있는 프로젝트 중 하나입니다. Langerve는 인공지능 기술을 활용하여 자연어 처리와 대화 생성 분야에서 혁신을 추구하는 언어 모델입니다.\n",
      "\n",
      "Langerve의 이름은 'Language Evolution'의 약자로, 인공지능이 인간과 유사한 언어를 이해하고 생성하는 능력을 개발하는 것을 목표로 합니다. 이 프로젝트는 자연어 처리(NLP) 분야에서 최신 기술인 트랜스포머 아키텍처를 기반으로 하며, 대규모 언어 모델을 훈련하여 다양한 언어 작업에서 우수한 성능을 발휘하도록 설계되었습니다.\n",
      "\n",
      "Langerve의 주요 특징은 다음과 같습니다:\n",
      "\n",
      "1. **대규모 언어 모델**: Langerve는 수백만 개의 매개변수를 가진 대규모 언어 모델을 사용하여 훈련되었습니다. 이를 통해 모델은 복잡한 언어 패턴과 관계를 학습할 수 있습니다.\n",
      "\n",
      "2. **트랜스포머 아키텍처**: Langerve는 트랜스포머 아키텍처를 기반으로 하며, 이는 자연어 처리 작업에서 탁월한 성능을 발휘하는 것으로 입증되었습니다. 트랜스포머는 자기 주의 메커니즘을 사용하여 입력 시퀀스의 다양한 부분 간의 관계를 모델링할 수 있습니다.\n",
      "\n",
      "3. **다양한 언어 작업 지원**: Langerve는 다양한 언어 작업을 지원하도록 설계되었습니다. 여기에는 텍스트 분류, 감정 분석, 질의 응답, 대화 생성 등이 포함됩니다.\n",
      "\n",
      "4. **지속적인 학습**: Langerve는 지속적인 학습을 통해 성능을 지속적으로 개선할 수 있습니다. 새로운 데이터가 추가되면 모델은 자동으로 업데이트되어 최신 언어 패턴과 지식을 습득할 수 있습니다.\n",
      "\n",
      "5. **확장성**: Langerve는 확장성이 뛰어난 모델입니다. 즉, 추가적인 훈련이나 미세 조정을 통해 특정 도메인이나 작업에 맞게 모델을 쉽게 조정할 수 있습니다.\n",
      "\n",
      "Langerve의 응용 분야는 매우 다양합니다. 예를 들어, 고객 서비스 챗봇, 언어 번역, 콘텐츠 생성, 감정 분석 등 다양한 분야에서 활용될 수 있습니다. 또한, Langerve는 장애인을 위한 언어 지원 도구 개발에도 사용될 수 있습니다.\n",
      "\n",
      "현재 Langerve는 개발 단계에 있으며, 지속적인 연구와 개발을 통해 성능을 개선하고 있습니다. 향후 Langerve는 다양한 산업 분야에서 널리 활용될 것으로 기대됩니다.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = llm.invoke(prompt_text)\n",
    "    #print(\"응답:\", response.content)\n",
    "    print(type(response))\n",
    "    print(response)\n",
    "    print(response.content)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1038a52d",
   "metadata": {},
   "source": [
    "LCEL\n",
    "- Prompt + LLM을 Chain으로 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e3f7038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='\\n    You are an expert in AI Expert. Answer the question. \\n    <Question>: {input}에 대해 쉽게 설명해주세요.\")\\n    ')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an expert in AI Expert. Answer the question. \n",
    "    <Question>: {input}에 대해 쉽게 설명해주세요.\")\n",
    "    \"\"\")                                     \n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6851accd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "# chain 연결 (LCEL)\n",
    "chain = prompt | llm\n",
    "print(type(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12bda93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# chain 연결 (LCEL)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain2 = prompt | llm | output_parser\n",
    "print(type(chain2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0f102cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='LangChain은 다양한 AI 관련 제품을 제공하는 회사입니다. 주요 제품은 다음과 같습니다:\\n\\n1. **LangSmith**: 랭스미스는 랭체인에서 제공하는 디버깅 및 모니터링 툴입니다. LangChain을 사용하여 빌드된 애플리케이션의 성능을 모니터링하고 디버깅하는데 사용됩니다.\\n\\n2. **LangServe**: 랭서브는 LangChain에서 제공하는 API 제공 툴입니다. LangChain으로 개발한 애플리케이션을 쉽게 API로 배포하고 관리할 수 있습니다.\\n\\n3. **LangChain**: 랭체인은 대규모 언어 모델(LLM)을 활용하여 자연어 처리 작업을 수행하는 프레임워크입니다. 개발자가 언어 모델을 쉽게 통합하고 활용할 수 있도록 지원합니다.\\n\\n이러한 제품들은 LangChain이 제공하는 다양한 솔루션의 일부입니다. LangChain은 개발자가 자연어 처리 및 인공지능 기술을 보다 쉽게 활용할 수 있도록 돕는 것을 목표로 합니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 183, 'prompt_tokens': 56, 'total_tokens': 239, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'queue_time': 0.38048327, 'prompt_time': 0.003705173, 'completion_time': 0.439663499, 'total_time': 0.443368672}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_79da0e0073', 'id': 'chatcmpl-27dbb987-3cf3-4d2d-9160-a1cb2d28b341', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--bc480a35-37b8-4f5d-81ee-8262c85be81c-0' usage_metadata={'input_tokens': 56, 'output_tokens': 183, 'total_tokens': 239, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "# chain 호출\n",
    "try:\n",
    "    result = chain.invoke({\"input\": \"LangChain의 Products (제품)는 어떤 것들이 있나요? 예를 들어 LangSmith, LangServe 같은 Product가 있어\"})\n",
    "    print(type(result))\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d077f356",
   "metadata": {},
   "source": [
    "Runnable의 stream() 함수 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3edf8347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인공지능 모델의 학습 원리를 쉽게 설명해 드리겠습니다.\n",
      "\n",
      "인공지능 모델의 학습 원리는 사람의 뇌가 학습하는 방식과 유사합니다. 사람의 뇌는 경험을 통해 학습하고, 인공지능 모델도 데이터를 통해 학습합니다.\n",
      "\n",
      "**단계 1: 데이터 수집**\n",
      "인공지능 모델을 학습시키기 위해서는大量的 데이터가 필요합니다. 이 데이터는 문제에 대한 답이 포함된 형태여야 합니다. 예를 들어, 고양이와 강아지의 사진을 분류하는 모델을 만든다고 가정해 봅시다. 이 경우, 고양이와 강아지의 사진 데이터와 각각의 사진에 대한 라벨(고양이 또는 강아지)이 필요합니다.\n",
      "\n",
      "**단계 2: 데이터 전처리**\n",
      "수집한 데이터를 모델이 학습할 수 있도록 가공하는 단계입니다. 데이터의 품질을 확인하고, 누락된 값을 채우고, 데이터를 정규화하는 등의 작업을 수행합니다.\n",
      "\n",
      "**단계 3: 모델 초기화**\n",
      "인공지능 모델을 초기화합니다. 모델은 입력층, 은닉층, 출력층으로 구성됩니다. 입력층은 데이터를 입력받는 층, 은닉층은 데이터를 처리하는 층, 출력층은 결과를 출력하는 층입니다.\n",
      "\n",
      "**단계 4: 순전파**\n",
      "입력층에 데이터를 입력하면, 데이터는 은닉층을 거쳐 출력층으로 전달됩니다. 이 과정에서 각 층의 가중치와 편향이 적용됩니다. 가중치와 편향은 모델이 학습하는 과정에서 조정됩니다.\n",
      "\n",
      "**단계 5: 손실 함수 계산**\n",
      "출력층에서 출력된 결과와 실제 답(라벨) 사이의 차이를 계산합니다. 이 차이를 손실(loss)이라고 하며, 손실 함수를 통해 계산합니다.\n",
      "\n",
      "**단계 6: 역전파**\n",
      "손실 함수를 통해 계산된 손실을 줄이기 위해, 출력층에서 입력층으로 오류를 역전파합니다. 이 과정에서 각 층의 가중치와 편향이 조정됩니다.\n",
      "\n",
      "**단계 7: 가중치 업데이트**\n",
      "역전파를 통해 각 층의 가중치와 편향이 조정됩니다. 이 과정은 최적화 알고리즘에 의해 수행되며, 가중치와 편향이 업데이트됩니다.\n",
      "\n",
      "**단계 8: 반복 학습**\n",
      "단계를 4번부터 7번까지 반복합니다. 반복할수록 모델의 성능이 개선됩니다.\n",
      "\n",
      "**단계 9: 모델 평가**\n",
      "학습이 완료되면, 모델의 성능을 평가합니다. 평가 데이터에 대해 모델의 성능을 측정하고, 필요에 따라 모델을 수정합니다.\n",
      "\n",
      "이렇게 인공지능 모델은 데이터를 통해 학습하고, 성능을 개선합니다. 학습 원리를 이해하면, 인공지능 모델을 더 잘 활용할 수 있습니다."
     ]
    }
   ],
   "source": [
    "# 스트리밍 출력을 위한 요청\n",
    "try:\n",
    "    answer = chain2.stream({\"input\": \"인공지능 모델의 학습 원리를 자세하게 설명해 주세요.\"})\n",
    "    \n",
    "    # 스트리밍 출력\n",
    "    #print(answer)\n",
    "    for token in answer:\n",
    "        # 스트림에서 받은 데이터의 내용을 출력합니다. 줄바꿈 없이 이어서 출력하고, 버퍼를 즉시 비웁니다.\n",
    "        print(token, end=\"\", flush=True)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fd7fdf",
   "metadata": {},
   "source": [
    "Multi Chain\n",
    "- 첫번째 Chain의 출력이, 두번째 Chain의 입력이 된다.\n",
    "- 두개의 Chain과 Prompt + OutputParser를 LCEL로 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "145596e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Step 1: 사용자가 입력한 장르에 따라 영화 추천\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{genre} 장르에서 추천할 만한 영화를 한 편 알려주세요.\")\n",
    "\n",
    "# Step 2: 추천된 영화의 줄거리를 요약\n",
    "prompt2 = ChatPromptTemplate.from_template(\"{movie} 추전한 영화의 제목을 먼저 알려주시고, 줄을 바꾸어서 영화의 줄거리를 3문장으로 요약해 주세요.\")\n",
    "\n",
    "# OpenAI 모델 사용\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 체인 1: 영화 추천 (입력: 장르 → 출력: 영화 제목)\n",
    "chain1 = prompt1 | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f6a2187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추천한 영화의 제목은 '겟 아웃'(Get Out)입니다.\n",
      "\n",
      "영화 '겟 아웃'의 줄거리입니다.\n",
      "\n",
      "영화는 흑인 사진 작가 크리스가 백인 여자친구 로즈의 집에 초대받으면서 시작됩니다. 크리스는 로즈의 가족과 친척들이 모두 백인이라는 점과 그들이 자신을 이상하게 대하는 점에 대해 불안감을 느끼기 시작합니다. 크리스는 로즈의 가족이 자신을 상대로 음모를 꾸미고 있다는 것을 깨닫고 탈출을 시도하게 됩니다.\n"
     ]
    }
   ],
   "source": [
    "# 체인 2: 줄거리 요약 (입력: 영화 제목 → 출력: 줄거리)\n",
    "try:\n",
    "    chain2 = (\n",
    "        {\"movie\": chain1}  # chain1의 출력을 movie 입력 변수로 전달\n",
    "        | prompt2\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # 실행: \"SF\" 장르의 영화 추천 및 줄거리 요약\n",
    "    response = chain2.invoke({\"genre\": \"공포\"})\n",
    "    print(response)  \n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9d96fa",
   "metadata": {},
   "source": [
    "PromptTemplate 결합하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf86400",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_text = \"{model_name} 모델의 학습 원리를 {count} 문장으로 한국어로 답변해 주세요.\"\n",
    "\n",
    "# PromptTemplate 인스턴스를 생성\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "# 템플릿에 값을 채워서 프롬프트를 완성\n",
    "filled_prompt = prompt_template.format(model_name=\"ChatGPT\", count=3)\n",
    "\n",
    "# 문자열 템플릿 결합 (PromptTemplate + PromptTemplate + 문자열)\n",
    "combined_prompt = (\n",
    "              prompt_template\n",
    "              + PromptTemplate.from_template(\"\\n\\n 그리고 {model_name} 모델의 장점을 요약 정리해 주세요\")\n",
    "              + \"\\n\\n {model_name} 모델과 비슷한 AI 모델은 어떤 것이 있나요? 모델명은 {language}로 답변해 주세요.\"\n",
    ")\n",
    "combined_prompt.format(model_name=\"ChatGPT\", count=3, language=\"영어\")\n",
    "\n",
    "\n",
    "# Groq API\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    #model=\"mistral-saba-24b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "chain = combined_prompt | llm | StrOutputParser()\n",
    "chain.invoke({\"model_name\":\"ChatGPT\", \"count\":3, \"language\":\"영어\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0fa9b3",
   "metadata": {},
   "source": [
    "여러개의 PromptTemplate 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd94d456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GPT-4 모델의 학습 원리를 2 문장으로 한국어로 답변해 주세요.', 'Gemma 모델의 학습 원리를 3 문장으로 한국어로 답변해 주세요.', 'llama-4 모델의 학습 원리를 4 문장으로 한국어로 답변해 주세요.']\n"
     ]
    }
   ],
   "source": [
    "template_text = \"{model_name} 모델의 학습 원리를 {count} 문장으로 한국어로 답변해 주세요.\"\n",
    "\n",
    "# PromptTemplate 인스턴스를 생성\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "questions = [\n",
    "    {\"model_name\": \"GPT-4\", \"count\": 2},\n",
    "    {\"model_name\": \"Gemma\", \"count\": 3},\n",
    "    {\"model_name\": \"llama-4\", \"count\": 4},\n",
    "]\n",
    "\n",
    "# 여러 개의 프롬프트를 미리 생성\n",
    "formatted_prompts = [prompt_template.format(**q) for q in questions]\n",
    "print(formatted_prompts)  # 미리 생성된 질문 목록 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "162ca951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4 모델은 대규모 데이터 세트에 대해 훈련된 생성형 언어 모델로서, 주어진 입력에 대해 다음에 올 가능성이 높은 단어를 예측하도록 학습합니다. GPT-4는 이전 모델인 GPT-3와 비교하여 더욱 많은 양의 데이터와 컴퓨팅 자원을 사용하여 학습되었기 때문에 향상된 성능과 자연스러운 언어 생성 능력을 갖추고 있습니다.\n",
      "Gemma는 컴퓨터가 자연어로 작성된 문서를 학습하고 이해할 수 있도록 지원하는 모델입니다. Gemma는 대규모 언어 모델을 학습하기 위한 다양한 기술을 사용하며, 훈련 데이터로 부터 학습하여  자연어 처리 작업을 보다 쉽게 수행할 수 있습니다. 예를 들어, Gemma는 문장 완성, 질문-답변, 자연어 이해와 같은 작업에 사용할 수 있습니다.\n",
      "llama-4 모델은 메타에서 개발한 대규모 언어 모델입니다. 이 모델은 방대한 양의 텍스트 데이터를 바탕으로 학습되며, 이를 통해 자연어 처리 능력을 습득합니다. 학습 과정에서는 주어진 문맥에서 다음에 올 단어를 예측하도록 훈련되며, 이 과정에서 모델의 가중치가 조정됩니다. 이처럼 llama-4 모델은 지속적인 학습을 통해 언어 이해 및 생성 능력을 향상시킵니다.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    #model=\"mistral-saba-24b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "for prompt in formatted_prompts:\n",
    "    response = llm.invoke(prompt) # AIMessage\n",
    "    print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cb33ef",
   "metadata": {},
   "source": [
    "### ChatMessagePromptTemplate\n",
    "- SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "190de84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "딥러닝은 인공신경망을 사용하여 데이터를 분석하고 패턴을 찾는 머신러닝의 한 분야입니다. 딥러닝은 데이터의 특징을 자동으로 학습하고, 이를 통해 이미지, 음성, 텍스트 등의 데이터를 분류, 예측, 생성하는 데 사용됩니다. 특히, 딥러닝은 빅데이터와 복잡한 패턴을 처리하는 데 뛰어난 성능을 발휘합니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 개별 메시지 템플릿 정의\n",
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "    \"당신은 {topic} 전문가입니다. 명확하고 자세하게 한국어로 설명해 주세요.\"\n",
    ")\n",
    "user_message = HumanMessagePromptTemplate.from_template(\n",
    "    \"{question}\"\n",
    ")\n",
    "ai_message = AIMessagePromptTemplate.from_template(\n",
    "    \"This is an example answer about {topic}.\"\n",
    ")\n",
    "\n",
    "# ChatPromptTemplate로 메시지들을 묶기\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    user_message,\n",
    "    ai_message\n",
    "])\n",
    "\n",
    "# 메시지 생성\n",
    "messages = chat_prompt.format_messages(topic=\"AI\", question=\"딥러닝은 무엇인가요?\")\n",
    "\n",
    "# LLM 호출\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "# 결과 출력\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564dd217",
   "metadata": {},
   "source": [
    "FewShotChatPromptTemplate 사용한 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a33358b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 태양계의 행성\n",
      "1. **수성**: 가장 작은 행성, 태양과 가장 가깝습니다.\n",
      "2. **금성**: 매우 뜨겁고 밝은 행성입니다.\n",
      "3. **지구**: 생명체가 살고 있는 유일한 행성입니다.\n",
      "4. **화성**: 붉은 행성으로, 로봇 탐사가 이루어지고 있습니다.\n",
      "5. **목성**: 태양계에서 가장 큰 행성입니다.\n",
      "6. **토성**: 아름다운 고리를 가지고 있습니다.\n",
      "7. **천왕성**: 자전축이 기울어져 있습니다.\n",
      "8. **해왕성**: 가장 먼 행성입니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"뉴턴의 운동 법칙을 요약해 주세요.\",\n",
    "        \"output\": \"\"\"### 뉴턴의 운동 법칙\n",
    "1. **관성의 법칙**: 힘이 작용하지 않으면 물체는 계속 같은 상태를 유지합니다.\n",
    "2. **가속도의 법칙**: 물체에 힘이 작용하면, 힘과 질량에 따라 가속도가 결정됩니다.\n",
    "3. **작용-반작용 법칙**: 모든 힘에는 크기가 같고 방향이 반대인 힘이 작용합니다.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"지구의 대기 구성 요소를 알려주세요.\",\n",
    "        \"output\": \"\"\"### 지구 대기의 구성\n",
    "- **질소 (78%)**: 대기의 대부분을 차지합니다.\n",
    "- **산소 (21%)**: 생명체가 호흡하는 데 필요합니다.\n",
    "- **아르곤 (0.93%)**: 반응성이 낮은 기체입니다.\n",
    "- **이산화탄소 (0.04%)**: 광합성 및 온실 효과에 중요한 역할을 합니다.\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 예제 프롬프트 템플릿\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# FewShotChatMessagePromptTemplate 적용\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# 최종 프롬프트 구성\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"당신은 초등학생도 이해할 수 있도록 쉽게 설명하는 과학 교육자입니다.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 모델 생성 및 체인 구성\n",
    "model = ChatOpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "chain = final_prompt | model\n",
    "\n",
    "# 테스트 실행\n",
    "result = chain.invoke({\"input\": \"태양계의 행성들을 간략히 정리해 주세요.\"})\n",
    "#result = chain.invoke({\"input\": \"양자 얽힘이 무엇인가요?\"})\n",
    "print(result.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-M3_YSb4G-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
